#!/usr/bin/env python3
"""
æ–‡æ¡£ä½¿ç”¨åˆ†æå·¥å…·
åˆ†ææ–‡æ¡£ä½¿ç”¨æ¨¡å¼ï¼Œå‘ç°æ”¹è¿›æœºä¼š
"""

import json
import os
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import re


class DocAnalytics:
    """æ–‡æ¡£åˆ†æå·¥å…·"""

    def __init__(self, docs_dir: Path):
        self.docs_dir = docs_dir
        self.analytics_file = docs_dir / "analytics.json"

    def analyze_content_structure(self):
        """åˆ†æå†…å®¹ç»“æ„"""
        analysis = {
            "pages": {},
            "total_words": 0,
            "code_blocks": 0,
            "external_links": 0,
            "images": 0,
            "tables": 0,
        }

        for rst_file in self.docs_dir.glob("*.rst"):
            with open(rst_file, "r", encoding="utf-8") as f:
                content = f.read()

            page_analysis = {
                "word_count": len(content.split()),
                "lines": len(content.split("\n")),
                "headers": len(re.findall(r'^[=\-~^"]{3,}$', content, re.MULTILINE)),
                "code_blocks": len(re.findall(r".. code-block::", content)),
                "external_links": len(re.findall(r"https?://", content)),
                "internal_refs": len(re.findall(r":doc:`[^`]+`", content)),
                "images": len(re.findall(r".. image::", content)),
                "tables": len(re.findall(r".. list-table::", content)),
                "tabs": len(re.findall(r".. tabs::", content)),
                "notes": len(re.findall(r".. note::", content)),
                "warnings": len(re.findall(r".. warning::", content)),
                "last_modified": datetime.fromtimestamp(
                    rst_file.stat().st_mtime
                ).isoformat(),
            }

            analysis["pages"][rst_file.name] = page_analysis
            analysis["total_words"] += page_analysis["word_count"]
            analysis["code_blocks"] += page_analysis["code_blocks"]
            analysis["external_links"] += page_analysis["external_links"]
            analysis["images"] += page_analysis["images"]
            analysis["tables"] += page_analysis["tables"]

        return analysis

    def calculate_readability_scores(self):
        """è®¡ç®—å¯è¯»æ€§åˆ†æ•°"""
        scores = {}

        for rst_file in self.docs_dir.glob("*.rst"):
            with open(rst_file, "r", encoding="utf-8") as f:
                content = f.read()

            # ç®€å•çš„å¯è¯»æ€§æŒ‡æ ‡
            sentences = len(re.findall(r"[.!?]+", content))
            words = len(content.split())
            complex_words = len(re.findall(r"\w{7,}", content))  # 7+å­—ç¬¦çš„è¯

            if sentences > 0:
                avg_sentence_length = words / sentences
                complex_word_ratio = complex_words / words if words > 0 else 0

                # ç®€åŒ–çš„å¯è¯»æ€§åˆ†æ•° (è¶Šä½è¶Šå¥½è¯»)
                readability_score = (avg_sentence_length * 1.5) + (
                    complex_word_ratio * 100
                )

                scores[rst_file.name] = {
                    "readability_score": round(readability_score, 2),
                    "avg_sentence_length": round(avg_sentence_length, 2),
                    "complex_word_ratio": round(complex_word_ratio, 3),
                    "rating": self._get_readability_rating(readability_score),
                }

        return scores

    def _get_readability_rating(self, score):
        """è·å–å¯è¯»æ€§ç­‰çº§"""
        if score < 20:
            return "Very Easy"
        elif score < 40:
            return "Easy"
        elif score < 60:
            return "Moderate"
        elif score < 80:
            return "Difficult"
        else:
            return "Very Difficult"

    def analyze_navigation_depth(self):
        """åˆ†æå¯¼èˆªæ·±åº¦å’Œç»“æ„"""
        structure = {"max_depth": 0, "toc_analysis": {}}

        index_file = self.docs_dir / "index.rst"
        if index_file.exists():
            with open(index_file, "r", encoding="utf-8") as f:
                content = f.read()

            # åˆ†æ toctree æ·±åº¦
            toctree_pattern = r".. toctree::\s*\n(?:\s+:maxdepth:\s+(\d+)\s*\n)?"
            matches = re.findall(toctree_pattern, content)

            depths = [int(match) for match in matches if match]
            if depths:
                structure["max_depth"] = max(depths)

        return structure

    def identify_content_gaps(self):
        """è¯†åˆ«å†…å®¹ç©ºç™½"""
        gaps = []

        # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºçš„æˆ–è¿‡çŸ­çš„é¡µé¢
        for rst_file in self.docs_dir.glob("*.rst"):
            with open(rst_file, "r", encoding="utf-8") as f:
                content = f.read()

            word_count = len(content.split())

            if word_count < 50:
                gaps.append(
                    {
                        "file": rst_file.name,
                        "issue": "Too short",
                        "word_count": word_count,
                        "priority": "high",
                    }
                )
            elif word_count < 100:
                gaps.append(
                    {
                        "file": rst_file.name,
                        "issue": "Very short",
                        "word_count": word_count,
                        "priority": "medium",
                    }
                )

            # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘ä»£ç ç¤ºä¾‹
            if (
                "tutorial" in rst_file.name.lower()
                or "example" in rst_file.name.lower()
            ):
                if ".. code-block::" not in content:
                    gaps.append(
                        {
                            "file": rst_file.name,
                            "issue": "No code examples",
                            "priority": "high",
                        }
                    )

        return gaps

    def suggest_improvements(self):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        structure = self.analyze_content_structure()
        readability = self.calculate_readability_scores()
        gaps = self.identify_content_gaps()

        suggestions = []

        # åŸºäºå†…å®¹åˆ†æçš„å»ºè®®
        if structure["total_words"] < 5000:
            suggestions.append(
                {
                    "category": "Content Volume",
                    "suggestion": "Consider expanding documentation content",
                    "priority": "medium",
                    "details": f"Total words: {structure['total_words']}",
                }
            )

        # åŸºäºå¯è¯»æ€§çš„å»ºè®®
        difficult_pages = [
            page
            for page, score in readability.items()
            if score.get("readability_score", 0) > 60
        ]

        if difficult_pages:
            suggestions.append(
                {
                    "category": "Readability",
                    "suggestion": "Simplify language in difficult pages",
                    "priority": "high",
                    "details": f"Difficult pages: {', '.join(difficult_pages)}",
                }
            )

        # åŸºäºå†…å®¹ç©ºç™½çš„å»ºè®®
        high_priority_gaps = [gap for gap in gaps if gap["priority"] == "high"]
        if high_priority_gaps:
            suggestions.append(
                {
                    "category": "Content Gaps",
                    "suggestion": "Address high-priority content gaps",
                    "priority": "high",
                    "details": f"{len(high_priority_gaps)} high-priority gaps found",
                }
            )

        # åŸºäºä»£ç ç¤ºä¾‹çš„å»ºè®®
        if structure["code_blocks"] < 10:
            suggestions.append(
                {
                    "category": "Examples",
                    "suggestion": "Add more code examples",
                    "priority": "medium",
                    "details": f"Current code blocks: {structure['code_blocks']}",
                }
            )

        return suggestions

    def generate_report(self):
        """ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š"""
        report = {
            "generated_at": datetime.now().isoformat(),
            "structure": self.analyze_content_structure(),
            "readability": self.calculate_readability_scores(),
            "navigation": self.analyze_navigation_depth(),
            "gaps": self.identify_content_gaps(),
            "suggestions": self.suggest_improvements(),
        }

        # ä¿å­˜æŠ¥å‘Š
        with open(self.analytics_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        return report


def print_analysis_summary(report):
    """æ‰“å°åˆ†ææ‘˜è¦"""
    print("ğŸ“Š æ–‡æ¡£åˆ†ææŠ¥å‘Š")
    print("=" * 30)

    structure = report["structure"]
    print(f"\nğŸ“„ å†…å®¹æ¦‚è§ˆ:")
    print(f"   é¡µé¢æ•°é‡: {len(structure['pages'])}")
    print(f"   æ€»è¯æ•°: {structure['total_words']:,}")
    print(f"   ä»£ç å—: {structure['code_blocks']}")
    print(f"   å¤–éƒ¨é“¾æ¥: {structure['external_links']}")

    readability = report["readability"]
    if readability:
        avg_score = sum(
            page["readability_score"] for page in readability.values()
        ) / len(readability)
        print(f"\nğŸ“– å¯è¯»æ€§:")
        print(f"   å¹³å‡åˆ†æ•°: {avg_score:.1f}")

        ratings = defaultdict(int)
        for page in readability.values():
            ratings[page["rating"]] += 1

        for rating, count in ratings.items():
            print(f"   {rating}: {count} é¡µ")

    gaps = report["gaps"]
    if gaps:
        print(f"\nâš ï¸  å†…å®¹ç©ºç™½:")
        high_priority = sum(1 for gap in gaps if gap["priority"] == "high")
        print(f"   é«˜ä¼˜å…ˆçº§: {high_priority}")
        print(f"   æ€»è®¡: {len(gaps)}")

    suggestions = report["suggestions"]
    if suggestions:
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        for suggestion in suggestions:
            priority_emoji = (
                "ğŸ”´"
                if suggestion["priority"] == "high"
                else "ğŸŸ¡" if suggestion["priority"] == "medium" else "ğŸŸ¢"
            )
            print(f"   {priority_emoji} {suggestion['suggestion']}")


def main():
    """ä¸»å‡½æ•°"""
    docs_dir = Path(__file__).parent
    analytics = DocAnalytics(docs_dir)

    print("ğŸ” æ­£åœ¨åˆ†ææ–‡æ¡£...")
    report = analytics.generate_report()

    print_analysis_summary(report)

    print(f"\nğŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {analytics.analytics_file}")


if __name__ == "__main__":
    main()
